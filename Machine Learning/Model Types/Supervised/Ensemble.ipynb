{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Ensemble-Learning\" data-toc-modified-id=\"Ensemble-Learning-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Ensemble Learning</a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Learning Outcomes</a></span></li><li><span><a href=\"#The-Data\" data-toc-modified-id=\"The-Data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>The Data</a></span></li><li><span><a href=\"#Voting-Classifiers\" data-toc-modified-id=\"Voting-Classifiers-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Voting Classifiers</a></span><ul class=\"toc-item\"><li><span><a href=\"#Majority-Vote-(Hard-Voting)\" data-toc-modified-id=\"Majority-Vote-(Hard-Voting)-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Majority Vote (Hard Voting)</a></span></li><li><span><a href=\"#Soft-Voting\" data-toc-modified-id=\"Soft-Voting-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Soft Voting</a></span></li></ul></li><li><span><a href=\"#Stacking\" data-toc-modified-id=\"Stacking-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Stacking</a></span></li><li><span><a href=\"#Bagging\" data-toc-modified-id=\"Bagging-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Bagging</a></span></li><li><span><a href=\"#Boosting\" data-toc-modified-id=\"Boosting-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Boosting</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Outcomes\n",
    "\n",
    "* Distinguishing different Voting Classifiers\n",
    "* Implementing both soft and hard Voting Classifiers in sklearn\n",
    "* Implementing Stacking Classifier in sklearn\n",
    "* Implementing Bagging Classifier in sklearn\n",
    "* Implementing Boosting Classifiers in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Ensemble methods work best when the predictors are as independent from one another as possible. One way to get diverse classifiers is to train them using very different algorithms. This increases the chance that they will make very different types of errors, improving the ensemble’s accuracy.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fare</th>\n",
       "      <th>pclass_1</th>\n",
       "      <th>pclass_2</th>\n",
       "      <th>pclass_3</th>\n",
       "      <th>sex_female</th>\n",
       "      <th>sex_male</th>\n",
       "      <th>sibsp_0</th>\n",
       "      <th>sibsp_1</th>\n",
       "      <th>sibsp_2</th>\n",
       "      <th>...</th>\n",
       "      <th>sibsp_8</th>\n",
       "      <th>parch_0</th>\n",
       "      <th>parch_1</th>\n",
       "      <th>parch_2</th>\n",
       "      <th>parch_3</th>\n",
       "      <th>parch_4</th>\n",
       "      <th>parch_5</th>\n",
       "      <th>parch_6</th>\n",
       "      <th>alone_False</th>\n",
       "      <th>alone_True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    age     fare  pclass_1  pclass_2  pclass_3  sex_female  sex_male  sibsp_0  \\\n",
       "0  22.0   7.2500         0         0         1           0         1        0   \n",
       "1  38.0  71.2833         1         0         0           1         0        0   \n",
       "2  26.0   7.9250         0         0         1           1         0        1   \n",
       "3  35.0  53.1000         1         0         0           1         0        0   \n",
       "4  35.0   8.0500         0         0         1           0         1        1   \n",
       "\n",
       "   sibsp_1  sibsp_2  ...  sibsp_8  parch_0  parch_1  parch_2  parch_3  \\\n",
       "0        1        0  ...        0        1        0        0        0   \n",
       "1        1        0  ...        0        1        0        0        0   \n",
       "2        0        0  ...        0        1        0        0        0   \n",
       "3        1        0  ...        0        1        0        0        0   \n",
       "4        0        0  ...        0        1        0        0        0   \n",
       "\n",
       "   parch_4  parch_5  parch_6  alone_False  alone_True  \n",
       "0        0        0        0            1           0  \n",
       "1        0        0        0            1           0  \n",
       "2        0        0        0            0           1  \n",
       "3        0        0        0            1           0  \n",
       "4        0        0        0            0           1  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from utils import load_titanic\n",
    "from sklearn.model_selection import train_test_split\n",
    "X,y = load_titanic()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=0)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Majority Vote (Hard Voting) \n",
    "\n",
    "Uses class labels for voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.84      0.84       139\n",
      "           1       0.74      0.74      0.74        84\n",
      "\n",
      "    accuracy                           0.80       223\n",
      "   macro avg       0.79      0.79      0.79       223\n",
      "weighted avg       0.80      0.80      0.80       223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Voting Classifier takes a list of estimators as input.\n",
    "# With voting = 'hard'\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "estimators = [('lr',LogisticRegression()), ('ac',AdaBoostClassifier()), ('et',ExtraTreesClassifier())]\n",
    "\n",
    "clf = VotingClassifier(estimators)\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Voting\n",
    "\n",
    "Uses class probabilities for voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.86      0.85       139\n",
      "           1       0.76      0.71      0.74        84\n",
      "\n",
      "    accuracy                           0.81       223\n",
      "   macro avg       0.80      0.79      0.79       223\n",
      "weighted avg       0.81      0.81      0.81       223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = VotingClassifier(estimators,voting='soft')\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.88      0.86       139\n",
      "           1       0.79      0.71      0.75        84\n",
      "\n",
      "    accuracy                           0.82       223\n",
      "   macro avg       0.81      0.80      0.81       223\n",
      "weighted avg       0.82      0.82      0.82       223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "clf = StackingClassifier(estimators)\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "```\n",
    "\n",
    "Bagging involves selecting random subsets from both the observations and the features to train multiple classifiers.\n",
    "This way of aggragating classifers is used to reduce the variance of a model by introducing randomness. It is usually applied to strong and complex models(low bias).\n",
    "\n",
    "An example of a BaggingClassifier is the Random Forest model, which aggragates the outputs of fully grown decision trees with random subsets of the data.\n",
    "\n",
    "All classifiers contribute to an equal extend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.89      0.87       139\n",
      "           1       0.81      0.74      0.77        84\n",
      "\n",
      "    accuracy                           0.83       223\n",
      "   macro avg       0.83      0.82      0.82       223\n",
      "weighted avg       0.83      0.83      0.83       223\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.83      0.82       139\n",
      "           1       0.71      0.65      0.68        84\n",
      "\n",
      "    accuracy                           0.77       223\n",
      "   macro avg       0.75      0.74      0.75       223\n",
      "weighted avg       0.76      0.77      0.76       223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Lets see if 10 bagged Decision Tree models are better than 1.\n",
    "# Note: In this toy example dataset is very small and does not allow for much variation\n",
    "\n",
    "tree10 = DecisionTreeClassifier(random_state=0)\n",
    "preds = BaggingClassifier(tree10, n_estimators=10, random_state=0).fit(X_train, y_train).predict(X_test)\n",
    "print(classification_report(y_test,preds))\n",
    "\n",
    "\n",
    "tree1 = DecisionTreeClassifier(random_state=0)\n",
    "preds = tree1.fit(X_train, y_train).predict(X_test)\n",
    "print(classification_report(y_test,preds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "Using a weighted majority vote.\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.83      0.84       139\n",
      "           1       0.74      0.76      0.75        84\n",
      "\n",
      "    accuracy                           0.81       223\n",
      "   macro avg       0.79      0.80      0.80       223\n",
      "weighted avg       0.81      0.81      0.81       223\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.87      0.86       139\n",
      "           1       0.78      0.75      0.76        84\n",
      "\n",
      "    accuracy                           0.83       223\n",
      "   macro avg       0.81      0.81      0.81       223\n",
      "weighted avg       0.82      0.83      0.82       223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost uses many weak learners and weights to produce strong predictions.\n",
    "# Default is using a single branch decision tree, so simply a decision.\n",
    "\n",
    "clf = AdaBoostClassifier()\n",
    "preds = clf.fit(X_train, y_train).predict(X_test)\n",
    "print(classification_report(y_test,preds))\n",
    "\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# GradientBoostingClassifier is an additive model meaning it adds new models as it trains\n",
    "# It uses gradient descent to update its parameters.\n",
    "# Usually a low learning rate with high number of trees are used for moderately large dataset.\n",
    "# The state-of-the-art gradient boosting algorithms are discussing in SOTA folder.\n",
    "clf = GradientBoostingClassifier()\n",
    "preds = clf.fit(X_train, y_train).predict(X_test)\n",
    "print(classification_report(y_test,preds))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
